{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. I used Visual Studio Code with the [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) and [Polyglot Notebooks](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode) to create this sample.\n",
    "1. I launched the project using Python 3.12.3 and used venv to manage the dependencies.\n",
    "\n",
    "**Install CUDA**\n",
    "1. https://developer.nvidia.com/cuda-downloads\n",
    "\n",
    "1. Validate by running the following command\n",
    "    ```\n",
    "    nvcc --version\n",
    "    ```\n",
    "\n",
    "**Install Torch**\n",
    "1. https://pytorch.org/get-started/locally/\n",
    "1. Select the appropriate options for your system\n",
    "    ```\n",
    "    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages.\n",
    "\n",
    "# Provides hardware acceleration for running PyTorch on GPUs\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install transformers\n",
    "# %pip install accelerate\n",
    "\n",
    "# provides access to GPU metrics that we will print\n",
    "# %pip install GPUtil\n",
    "\n",
    "# provides access to distutils in Python 3.12\n",
    "# %pip install setuptools\n",
    "\n",
    "# allows for running python in VS Code Jupyter Notebooks\n",
    "# %pip install ipykernel\n",
    "\n",
    "# %pip freeze > requirements-using-phi1-inmemory.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Run the following code to validate the installation.\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cuda_is_available? True\n",
      "Using: NVIDIA RTX A2000 Laptop GPU \n"
     ]
    }
   ],
   "source": [
    "# validate that Cuda is available\n",
    "import torch\n",
    "\n",
    "print()\n",
    "\n",
    "# print the cuda device name\n",
    "print(\"Cuda_is_available? {}\\nUsing: {} \".format(\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\ai\\phi2-fine-tuning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# the model id is the model path on the Hugging Face model hub,\n",
    "# you can find it in the model's page URL\n",
    "base_model_id = \"microsoft/phi-1\"\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# AutoModelForCausalLM: This is a class from the Hugging Face Transformers library. It’s used\n",
    "#    for causal language modeling (LLM) tasks. Specifically, it’s designed for autoregressive\n",
    "#    generation, where the model predicts the next token in a sequence given the previous tokens.\n",
    "#\n",
    "# from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map={\"\": 0}):\n",
    "#    base_model_id: This parameter specifies the pretrained model to load. You provide either\n",
    "#       a shortcut name (e.g., 'bert-base-uncased') or a path to a directory containing a saved\n",
    "#       configuration file.\n",
    "#    trust_remote_code=True: This flag allows the model to download weights/configurations \n",
    "#       from a remote source (like Hugging Face’s model hub) if they are not already cached locally.\n",
    "#    torch_dtype=torch.float16: This sets the data type for the model’s weights to 16-bit\n",
    "#       floating point (half precision). This can help reduce memory usage and speed up inference.\n",
    "#    device_map={\"\": 0}: This maps the model to a specific device (in this case, device index 0).\n",
    "#       An empty string \"\" means the default device (usually CPU or GPU).\n",
    "\n",
    "# this line of code initializes an autoregressive language model (AutoModelForCausalLM) using pretrained weights specified by base_model_id\n",
    "model =  AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use a tokenizer to communicate with the model. The model doesn't understand our text, it understands tokens. The tokenizer converts our text into tokens and the model converts the tokens into predictions. The tokenizer is a crucial part of the model and it is important to use the same tokenizer that was used to train the model. The tokenizer is part of the model configuration and we can access it using `model.config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# AutoTokenizer: This is a class from the Hugging Face Transformers library. It’s used for tokenizing\n",
    "#    text data. Tokenization involves breaking down a sequence of text into individual tokens (words,\n",
    "#    subwords, or characters) for further processing by language models.\n",
    "#\n",
    "# from_pretrained(base_model_id, use_fast=True):\n",
    "#    base_model_id: This parameter specifies the pretrained model to load. You provide either a\n",
    "#       shortcut name (e.g., 'bert-base-uncased') or a path to a directory containing a saved\n",
    "#       configuration file.\n",
    "#    use_fast=True: This flag determines whether to use a fast Rust-based tokenizer if it’s supported\n",
    "#       for the given model. If a fast tokenizer is not available, a normal Python-based tokenizer is\n",
    "#       used instead.\n",
    "\n",
    "# this line of code initializes a tokenizer (AutoTokenizer) using pretrained weights specified by base_model_id.\n",
    "# The use_fast=True option indicates that it should use a faster tokenizer implementation if possible\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\ai\\phi2-fine-tuning\\.venv\\Lib\\site-packages\\torch\\utils\\_device.py:78: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for num in range(2, n+1):\n",
      "       for i in range(2, num):\n",
      "           if num % i == 0:\n",
      "               break\n",
      "       else:\n",
      "           print(num)\n",
      "\n",
      "\n",
      "\n",
      "from typing import List\n",
      "\n",
      "def find_smallest_multiple_of_list(li: List[int]) -> int:\n",
      "    \"\"\"\n",
      "    Returns the smallest positive integer that is divisible by all the numbers in the input list.\n",
      "\n",
      "    Args:\n",
      "    li (List[int]): A list of integers.\n",
      "\n",
      "    Returns:\n",
      "    int: The smallest positive integer that is divisible by all the numbers in the input list.\n",
      "    \"\"\"\n",
      "\n",
      "    # Find the maximum number in the list\n",
      "    max_num = max(li)\n",
      "\n",
      "    # Initialize the result to the maximum number\n",
      "    result = max_num\n",
      "\n",
      "    # Keep incrementing the result by the maximum number until it is divisible by all the numbers in the list\n",
      "    while True:\n",
      "        divisible = True\n",
      "        for num in li:\n",
      "            if result % num!= 0:\n",
      "                divisible = False\n",
      "                break\n",
      "        if divisible:\n",
      "            return result\n",
      "        result += max_num\n",
      "\n",
      "\n",
      "\n",
      "from typing import List\n",
      "\n",
      "def find_smallest_multiple_of_list(li: List[int]) -> int:\n",
      "    \"\"\"\n",
      "    Returns the smallest positive integer that is divisible by all the numbers in the input list.\n",
      "\n",
      "    Args:\n",
      "    li (List[int]): A list of integers.\n",
      "\n",
      "    Returns:\n",
      "    int: The smallest positive integer that is divisible by all the numbers in the input list.\n",
      "    \"\"\"\n",
      "\n",
      "    # Find the maximum number in the list\n",
      "    max_num = max(li)\n",
      "\n",
      "    # Initialize the result to the maximum number\n",
      "    result = max_num\n",
      "\n",
      "    # Keep incrementing the result by the maximum number until it is divisible by all the numbers in the list\n",
      "    while True:\n",
      "        divisible = True\n",
      "        for num in li:\n",
      "            if result % num!= 0:\n",
      "                divisible = False\n",
      "                break\n",
      "        if divisible:\n",
      "            return result\n",
      "        result += max_num\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's test the model by generating a function that prints all prime numbers between 1 and n\n",
    "prompt = '''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"'''\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "output = model.generate(**model_inputs, max_length=500)[0]\n",
    "\n",
    "# and finally, we print the output\n",
    "print(tokenizer.decode(output, skip_special_tokens=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "5\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# replace the code in the following block with the code provided by the model\n",
    "\n",
    "#BEGIN CODE BLOCK\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"\n",
    "   for num in range(2, n+1):\n",
    "       for i in range(2, num):\n",
    "           if num % i == 0:\n",
    "               break\n",
    "       else:\n",
    "           print(num)\n",
    "#END CODE BLOCK\n",
    "\n",
    "print_prime(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt --- 21.881 tokens/seconds ---\n",
      "GPU 0: NVIDIA RTX A2000 Laptop GPU, Utilization: 78.00%\n",
      "None\n",
      "Cite 20 famous people.\n",
      "\n",
      "    Args:\n",
      "    - people: A list of strings representing the names of the people in the list.\n",
      "\n",
      "    Returns:\n",
      "    - A string representing the name of the person who is the most famous among the people in the list.\n",
      "    \"\"\"\n",
      "    famous_people = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Heidi\", \"Ivan\", \"Judy\", \"Kevin\", \"Linda\", \"Mallory\", \"Nancy\", \"Oscar\", \"Peggy\", \"Quentin\", \"Romeo\", \"Sybil\", \"Trent\", \"Ursula\", \"Victor\", \"Wendy\", \"Xavier\", \"Yvonne\", \"Zoe\"]\n",
      "    cite_counts = {}\n",
      "    for person in people:\n",
      "        if person in famous_people:\n",
      "            if person in cite_counts:\n",
      "                cite_counts[person] += 1\n",
      "            else:\n",
      "                cite_counts[person] = 1\n",
      "    if not cite_counts:\n",
      "        return \"No famous people found in the list.\"\n",
      "    return max(cite_counts, key=cite_counts.get)\n",
      "\n",
      "\n",
      "\n",
      "from typing import List\n",
      "\n",
      "def find_most_common_letter(words: List[str]) -> str:\n",
      "    \"\"\"\n",
      "    Returns the most common letter among all the words in the input list.\n",
      "    If there are multiple letters with the same highest frequency, the function\n",
      "    returns the one that appears first in the alphabet.\n",
      "\n",
      "    Args:\n",
      "    - words: A list of strings.\n",
      "\n",
      "    Returns:\n",
      "    - A string representing the most common letter among all the words in the input list.\n",
      "    \"\"\"\n",
      "    letter_count = {}\n",
      "    for word in words:\n",
      "        for letter in word:\n",
      "            if letter in letter_count:\n",
      "                letter_count[letter] += 1\n",
      "            else:\n",
      "                letter_count[letter] = 1\n",
      "    max_count = max(letter_count.values())\n",
      "    most_common_letters = [letter for letter, count in letter_count.items() if count == max_count]\n",
      "    return min(most_common_letters)\n",
      "\n",
      "\n",
      "\n",
      "from\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import GPUtil\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    \"\"\"Prints GPU usage using GPUtil.\"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU {gpu.id}: {gpu.name}, Utilization: {gpu.load * 100:.2f}%\")\n",
    "\n",
    "# some text we want to send to the model to start our conversation\n",
    "prompt = \"Cite 20 famous people.\"\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "start_time = time.time()\n",
    "output = model.generate(**model_inputs, max_length=500)[0]\n",
    "tok_sec_prompt = round(len(output)/float(time.time() - start_time),3)\n",
    "print(\"Prompt --- %s tokens/seconds ---\" % (tok_sec_prompt))\n",
    "print(print_gpu_utilization())\n",
    "\n",
    "# and finally, we print the output\n",
    "print(tokenizer.decode(output, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
