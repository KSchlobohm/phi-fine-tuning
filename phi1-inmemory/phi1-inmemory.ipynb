{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. I used Visual Studio Code with the [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) and [Polyglot Notebooks](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode) to create this sample.\n",
    "1. I launched the project using Python 3.12.3 and used venv to manage the dependencies.\n",
    "\n",
    "**Install CUDA**\n",
    "1. https://developer.nvidia.com/cuda-downloads\n",
    "\n",
    "1. Validate by running the following command\n",
    "    ```\n",
    "    nvcc --version\n",
    "    ```\n",
    "\n",
    "**Install Torch**\n",
    "1. https://pytorch.org/get-started/locally/\n",
    "1. Select the appropriate options for your system\n",
    "    ```\n",
    "    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages.\n",
    "\n",
    "# Provides hardware acceleration for running PyTorch on GPUs\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Provide APIs and tools to easily download and train state-of-the-art pre-trained models.\n",
    "# %pip install transformers\n",
    "\n",
    "# A library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code!\n",
    "# %pip install accelerate\n",
    "\n",
    "# provides access to GPU metrics that we will print\n",
    "# %pip install GPUtil\n",
    "\n",
    "# provides access to distutils in Python 3.12\n",
    "# %pip install setuptools\n",
    "\n",
    "# allows for running python in VS Code Jupyter Notebooks\n",
    "# %pip install ipykernel\n",
    "\n",
    "# %pip freeze > requirements-using-phi1-inmemory.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to validate the installation.\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate that Cuda is available\n",
    "import torch\n",
    "\n",
    "print()\n",
    "\n",
    "# print the cuda device name\n",
    "print(\"Cuda_is_available? {}\\nUsing: {} \".format(\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# the model id is the model path on the Hugging Face model hub,\n",
    "# you can find it in the model's page URL\n",
    "base_model_id = \"microsoft/phi-1\"\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# AutoModelForCausalLM: This is a class from the Hugging Face Transformers library. It’s used\n",
    "#    for causal language modeling (LLM) tasks. Specifically, it’s designed for autoregressive\n",
    "#    generation, where the model predicts the next token in a sequence given the previous tokens.\n",
    "#\n",
    "# from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map={\"\": 0}):\n",
    "#    base_model_id: This parameter specifies the pretrained model to load. You provide either\n",
    "#       a shortcut name (e.g., 'bert-base-uncased') or a path to a directory containing a saved\n",
    "#       configuration file.\n",
    "#    trust_remote_code=True: This flag allows the model to download weights/configurations \n",
    "#       from a remote source (like Hugging Face’s model hub) if they are not already cached locally.\n",
    "#    torch_dtype=torch.float16: This sets the data type for the model’s weights to 16-bit\n",
    "#       floating point (half precision). This can help reduce memory usage and speed up inference.\n",
    "#    device_map={\"\": 0}: This maps the model to a specific device (in this case, device index 0).\n",
    "#       An empty string \"\" means the default device (usually CPU or GPU).\n",
    "\n",
    "# this line of code initializes an autoregressive language model (AutoModelForCausalLM) using pretrained weights specified by base_model_id\n",
    "model =  AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use a tokenizer to communicate with the model. The model doesn't understand our text, it understands tokens. The tokenizer converts our text into tokens and the model converts the tokens into predictions. The tokenizer is a crucial part of the model and it is important to use the same tokenizer that was used to train the model. The tokenizer is part of the model configuration and we can access it using `model.config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# AutoTokenizer: This is a class from the Hugging Face Transformers library. It’s used for tokenizing\n",
    "#    text data. Tokenization involves breaking down a sequence of text into individual tokens (words,\n",
    "#    subwords, or characters) for further processing by language models.\n",
    "#\n",
    "# from_pretrained(base_model_id, use_fast=True):\n",
    "#    base_model_id: This parameter specifies the pretrained model to load. You provide either a\n",
    "#       shortcut name (e.g., 'bert-base-uncased') or a path to a directory containing a saved\n",
    "#       configuration file.\n",
    "#    use_fast=True: This flag determines whether to use a fast Rust-based tokenizer if it’s supported\n",
    "#       for the given model. If a fast tokenizer is not available, a normal Python-based tokenizer is\n",
    "#       used instead.\n",
    "\n",
    "# this line of code initializes a tokenizer (AutoTokenizer) using pretrained weights specified by base_model_id.\n",
    "# The use_fast=True option indicates that it should use a faster tokenizer implementation if possible\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test the model by generating a function that prints all prime numbers between 1 and n\n",
    "prompt = '''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"'''\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "output = model.generate(**model_inputs, max_length=500)[0]\n",
    "\n",
    "# and finally, we print the output\n",
    "print(tokenizer.decode(output, skip_special_tokens=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the code in the following block with the code provided by the model\n",
    "\n",
    "#BEGIN CODE BLOCK\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"\n",
    "   for num in range(2, n+1):\n",
    "       for i in range(2, num):\n",
    "           if num % i == 0:\n",
    "               break\n",
    "       else:\n",
    "           print(num)\n",
    "#END CODE BLOCK\n",
    "\n",
    "print_prime(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import GPUtil\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    \"\"\"Prints GPU usage using GPUtil.\"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU {gpu.id}: {gpu.name}, Utilization: {gpu.load * 100:.2f}%\")\n",
    "\n",
    "# some text we want to send to the model to start our conversation\n",
    "prompt = \"Cite 20 famous people.\"\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "start_time = time.time()\n",
    "output = model.generate(**model_inputs, max_length=500)[0]\n",
    "tok_sec_prompt = round(len(output)/float(time.time() - start_time),3)\n",
    "print(\"Prompt --- %s tokens/seconds ---\" % (tok_sec_prompt))\n",
    "print(print_gpu_utilization())\n",
    "\n",
    "# and finally, we print the output\n",
    "print(tokenizer.decode(output, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
