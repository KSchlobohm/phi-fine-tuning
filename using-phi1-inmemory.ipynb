{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. I used Visual Studio Code with the [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) and [Polyglot Notebooks](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode) to create this sample.\n",
    "1. I launched the project using Python 3.12.3 and used venv to manage the dependencies.\n",
    "\n",
    "**Install CUDA**\n",
    "1. https://developer.nvidia.com/cuda-downloads\n",
    "\n",
    "1. Validate by running the following command\n",
    "    ```\n",
    "    nvcc --version\n",
    "    ```\n",
    "\n",
    "**Install Torch**\n",
    "1. https://pytorch.org/get-started/locally/\n",
    "1. Select the appropriate options for your system\n",
    "    ```\n",
    "    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    ```\n",
    "\n",
    "**Validate**\n",
    "\n",
    "Run the following code to validate the installation.\n",
    "```\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages.\n",
    "\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install transformers\n",
    "# %pip install accelerate\n",
    "# %pip install ipykernel\n",
    "\n",
    "# %pip freeze > requirements-using-phi1-inmemory.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\ai\\phi2-fine-tuning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\dev\\ai\\phi2-fine-tuning\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\dev\\ai\\phi2-fine-tuning\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# the model id is the model path on the Hugging Face model hub,\n",
    "# you can find it in the model's page URL\n",
    "base_model_id = \"microsoft/phi-1\"\n",
    "\n",
    "# AutoModelForCausalLM: This is a class from the Hugging Face Transformers library. It’s used\n",
    "#    for causal language modeling (LLM) tasks. Specifically, it’s designed for autoregressive\n",
    "#    generation, where the model predicts the next token in a sequence given the previous tokens.\n",
    "#\n",
    "# from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map={\"\": 0}):\n",
    "#    base_model_id: This parameter specifies the pretrained model to load. You provide either\n",
    "#       a shortcut name (e.g., 'bert-base-uncased') or a path to a directory containing a saved\n",
    "#       configuration file.\n",
    "#    trust_remote_code=True: This flag allows the model to download weights/configurations \n",
    "#       from a remote source (like Hugging Face’s model hub) if they are not already cached locally.\n",
    "#    torch_dtype=torch.float16: This sets the data type for the model’s weights to 16-bit\n",
    "#       floating point (half precision). This can help reduce memory usage and speed up inference.\n",
    "#    device_map={\"\": 0}: This maps the model to a specific device (in this case, device index 0).\n",
    "#       An empty string \"\" means the default device (usually CPU or GPU).\n",
    "\n",
    "# this line of code initializes an autoregressive language model (AutoModelForCausalLM) using pretrained weights specified by base_model_id\n",
    "model =  AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use a tokenizer to communicate with the model. The model doesn't understand our text, it understands tokens. The tokenizer converts our text into tokens and the model converts the tokens into predictions. The tokenizer is a crucial part of the model and it is important to use the same tokenizer that was used to train the model. The tokenizer is part of the model configuration and we can access it using `model.config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# AutoTokenizer: This is a class from the Hugging Face Transformers library. It’s used for tokenizing\n",
    "#    text data. Tokenization involves breaking down a sequence of text into individual tokens (words,\n",
    "#    subwords, or characters) for further processing by language models.\n",
    "#\n",
    "# from_pretrained(base_model_id, use_fast=True):\n",
    "#    base_model_id: This parameter specifies the pretrained model to load. You provide either a\n",
    "#       shortcut name (e.g., 'bert-base-uncased') or a path to a directory containing a saved\n",
    "#       configuration file.\n",
    "#    use_fast=True: This flag determines whether to use a fast Rust-based tokenizer if it’s supported\n",
    "#       for the given model. If a fast tokenizer is not available, a normal Python-based tokenizer is\n",
    "#       used instead.\n",
    "\n",
    "# this line of code initializes a tokenizer (AutoTokenizer) using pretrained weights specified by base_model_id.\n",
    "# The use_fast=True option indicates that it should use a faster tokenizer implementation if possible\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\ai\\phi2-fine-tuning\\.venv\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:715: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cite 20 famous people.\n",
      "\n",
      "    Args:\n",
      "    - people: A list of strings representing the names of the people in the list.\n",
      "\n",
      "    Returns:\n",
      "    - A string representing the name of the person who is the most famous among the people in the list.\n",
      "    \"\"\"\n",
      "    famous_people = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Heidi\", \"Ivan\", \"Judy\", \"Kevin\", \"Linda\", \"Mallory\", \"Nancy\", \"Oscar\", \"Peggy\", \"Quentin\", \"Romeo\", \"Sybil\", \"Trent\", \"Ursula\", \"Victor\", \"Wendy\", \"Xavier\", \"Yvonne\", \"Zoe\"]\n",
      "    cite_counts = {}\n",
      "    for person in people:\n",
      "        if person in famous_people:\n",
      "            if person in cite_counts:\n",
      "                cite_counts[person] += 1\n",
      "            else:\n",
      "                cite_counts[person] = 1\n",
      "    if not cite_counts:\n",
      "        return \"No famous people found in the list.\"\n",
      "    return max(cite_counts, key=cite_counts.get)\n",
      "\n",
      "\n",
      "\n",
      "from typing import List\n",
      "\n",
      "def find_most_common_letter(words: List[str]) -> str:\n",
      "    \"\"\"\n",
      "    Returns the most common letter among all the words in the input list.\n",
      "    If there are multiple letters with the same highest frequency, the function\n",
      "    returns the one that appears first in the alphabet.\n",
      "\n",
      "    Args:\n",
      "    - words: A list of strings.\n",
      "\n",
      "    Returns:\n",
      "    - A string representing the most common letter among all the words in the input list.\n",
      "    \"\"\"\n",
      "    letter_count = {}\n",
      "    for word in words:\n",
      "        for letter in word:\n",
      "            if letter in letter_count:\n",
      "                letter_count[letter] += 1\n",
      "            else:\n",
      "                letter_count[letter] = 1\n",
      "    max_count = max(letter_count.values())\n",
      "    most_common_letters = [letter for letter, count in letter_count.items() if count == max_count]\n",
      "    return min(most_common_letters)\n",
      "\n",
      "\n",
      "\n",
      "from\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# some text we want to send to the model to start our conversation\n",
    "prompt = \"Cite 20 famous people.\"\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "output = model.generate(**model_inputs, max_length=500)[0]\n",
    "\n",
    "# and finally, we print the output\n",
    "print(tokenizer.decode(output, skip_special_tokens=True)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
