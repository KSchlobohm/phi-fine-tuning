{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. I used Visual Studio Code with the [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) and [Polyglot Notebooks](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode) to create this sample.\n",
    "1. I launched the project using Python 3.12.3 and used venv to manage the dependencies.\n",
    "\n",
    "**Install CUDA**\n",
    "1. https://developer.nvidia.com/cuda-downloads\n",
    "\n",
    "1. Validate by running the following command\n",
    "    ```\n",
    "    nvcc --version\n",
    "    ```\n",
    "\n",
    "**Install Torch**\n",
    "1. https://pytorch.org/get-started/locally/\n",
    "1. Select the appropriate options for your system\n",
    "    ```\n",
    "    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    ```\n",
    "\n",
    "**Validate**\n",
    "\n",
    "Run the following code to validate the installation.\n",
    "```\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages.\n",
    "\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install transformers\n",
    "# %pip install accelerate\n",
    "# %pip install ipykernel\n",
    "\n",
    "# %pip freeze > requirements-using-phi1-inmemory.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m      6\u001b[0m base_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# AutoModelForCausalLM: This is a class from the Hugging Face Transformers library. It’s used\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#    for causal language modeling (LLM) tasks. Specifically, it’s designed for autoregressive\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#    generation, where the model predicts the next token in a sequence given the previous tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# this line of code initializes an autoregressive language model (AutoModelForCausalLM) using pretrained weights specified by base_model_id\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m  \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\dev\\ai\\phi2-fine-tuning\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\dev\\ai\\phi2-fine-tuning\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3086\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3083\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3084\u001b[0m         )\n\u001b[0;32m   3085\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 3086\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   3087\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3088\u001b[0m         )\n\u001b[0;32m   3090\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[0;32m   3091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[1;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# the model id is the model path on the Hugging Face model hub,\n",
    "# you can find it in the model's page URL\n",
    "base_model_id = \"microsoft/phi-1\"\n",
    "\n",
    "# AutoModelForCausalLM: This is a class from the Hugging Face Transformers library. It’s used\n",
    "#    for causal language modeling (LLM) tasks. Specifically, it’s designed for autoregressive\n",
    "#    generation, where the model predicts the next token in a sequence given the previous tokens.\n",
    "#\n",
    "# from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map={\"\": 0}):\n",
    "#    base_model_id: This parameter specifies the pretrained model to load. You provide either\n",
    "#       a shortcut name (e.g., 'bert-base-uncased') or a path to a directory containing a saved\n",
    "#       configuration file.\n",
    "#    trust_remote_code=True: This flag allows the model to download weights/configurations \n",
    "#       from a remote source (like Hugging Face’s model hub) if they are not already cached locally.\n",
    "#    torch_dtype=torch.float16: This sets the data type for the model’s weights to 16-bit\n",
    "#       floating point (half precision). This can help reduce memory usage and speed up inference.\n",
    "#    device_map={\"\": 0}: This maps the model to a specific device (in this case, device index 0).\n",
    "#       An empty string \"\" means the default device (usually CPU or GPU).\n",
    "\n",
    "# this line of code initializes an autoregressive language model (AutoModelForCausalLM) using pretrained weights specified by base_model_id\n",
    "model =  AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use a tokenizer to communicate with the model. The model doesn't understand our text, it understands tokens. The tokenizer converts our text into tokens and the model converts the tokens into predictions. The tokenizer is a crucial part of the model and it is important to use the same tokenizer that was used to train the model. The tokenizer is part of the model configuration and we can access it using `model.config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# AutoTokenizer: This is a class from the Hugging Face Transformers library. It’s used for tokenizing\n",
    "#    text data. Tokenization involves breaking down a sequence of text into individual tokens (words,\n",
    "#    subwords, or characters) for further processing by language models.\n",
    "#\n",
    "# from_pretrained(base_model_id, use_fast=True):\n",
    "#    base_model_id: This parameter specifies the pretrained model to load. You provide either a\n",
    "#       shortcut name (e.g., 'bert-base-uncased') or a path to a directory containing a saved\n",
    "#       configuration file.\n",
    "#    use_fast=True: This flag determines whether to use a fast Rust-based tokenizer if it’s supported\n",
    "#       for the given model. If a fast tokenizer is not available, a normal Python-based tokenizer is\n",
    "#       used instead.\n",
    "\n",
    "# this line of code initializes a tokenizer (AutoTokenizer) using pretrained weights specified by base_model_id.\n",
    "# The use_fast=True option indicates that it should use a faster tokenizer implementation if possible\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cite 20 famous people.\n",
      "\n",
      "    Args:\n",
      "    - people: A list of strings representing the names of the people in the list.\n",
      "\n",
      "    Returns:\n",
      "    - A string representing the name of the person who is the most famous among the people in the list.\n",
      "    \"\"\"\n",
      "    famous_people = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Heidi\", \"Ivan\", \"Judy\", \"Kevin\", \"Linda\", \"Mallory\", \"Nancy\", \"Oscar\", \"Peggy\", \"Quentin\", \"Romeo\", \"Sybil\", \"Trent\", \"Ursula\", \"Victor\", \"Wendy\", \"Xavier\", \"Yvonne\", \"Zoe\"]\n",
      "    cite_counts = {}\n",
      "    for person in people:\n",
      "        if person in famous_people:\n",
      "            if person in cite_counts:\n",
      "                cite_counts[person] += 1\n",
      "            else:\n",
      "                cite_counts[person] = 1\n",
      "    if not cite_counts:\n",
      "        return \"No famous people found in the list.\"\n",
      "    return max(cite_counts, key=cite_counts.get)\n",
      "\n",
      "\n",
      "\n",
      "from typing import List\n",
      "\n",
      "def find_most_common_letter(words: List[str]) -> str:\n",
      "    \"\"\"\n",
      "    Returns the most common letter among all the words in the input list.\n",
      "    If there are multiple letters with the same highest frequency, the function\n",
      "    returns the one that appears first in the alphabet.\n",
      "\n",
      "    Args:\n",
      "    - words: A list of strings.\n",
      "\n",
      "    Returns:\n",
      "    - A string representing the most common letter among all the words in the input list.\n",
      "    \"\"\"\n",
      "    letter_count = {}\n",
      "    for word in words:\n",
      "        for letter in word:\n",
      "            if letter in letter_count:\n",
      "                letter_count[letter] += 1\n",
      "            else:\n",
      "                letter_count[letter] = 1\n",
      "    max_count = max(letter_count.values())\n",
      "    most_common_letters = [letter for letter, count in letter_count.items() if count == max_count]\n",
      "    return min(most_common_letters)\n",
      "\n",
      "\n",
      "\n",
      "from\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# some text we want to send to the model to start our conversation\n",
    "prompt = \"Cite 20 famous people.\"\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "output = model.generate(**model_inputs, max_length=500)[0]\n",
    "\n",
    "# and finally, we print the output\n",
    "print(tokenizer.decode(output, skip_special_tokens=True)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
