{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "1. See ph1-inmemory for cuda setup\n",
    "2. Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accessible large language models via k-bit quantization for PyTorch\n",
    "#%pip install bitsandbytes\n",
    "# a library for easily accessing, sharing, and processing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks\n",
    "#%pip install datasets\n",
    "# stands for Parameter-Efficient Fine-Tuning is a library for efficiently adapting large pre-trained models to various downstream applications without fine-tuning all of a modelâ€™s parameters because it is prohibitively costly\n",
    "#%pip install peft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install setuptools GPUtil datasets bitsandbytes accelerate trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate that Cuda is available\n",
    "import torch\n",
    "\n",
    "# print the cuda device name\n",
    "print(\"Cuda_is_available? {}\\nUsing: {} \".format(\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a helper function to print GPU utilization transformers\n",
    "import GPUtil\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    \"\"\"Prints GPU usage using GPUtil.\"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU {gpu.id}: {gpu.name}, Utilization: {gpu.load * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_id = \"microsoft/phi-1\"\n",
    "\n",
    "# from transformers import AutoModel\n",
    "# checkpoint_directory = \"./fine-tuned-model/checkpoint-500\"\n",
    "# \n",
    "# model_in_progress = AutoModel.from_pretrained(checkpoint_directory, load_in_8bit=True)\n",
    "# fine_tuned_tokenizer = AutoTokenizer.from_pretrained(checkpoint_directory)\n",
    "\n",
    "\n",
    "# this line of code initializes a tokenizer (AutoTokenizer) using pretrained weights specified by base_model_id.\n",
    "# The use_fast=True option indicates that it should use a faster tokenizer implementation if possible\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BitsAndBytesConfig class is a wrapper class that provides configuration options for working with models that have been loaded using bitsandbytes. It allows you to specify various attributes and features for quantization and loading models in different bit formats.\n",
    "\n",
    "In the provided code, the BitsAndBytesConfig object is initialized with several arguments. The load_in_4bit argument is set to True, indicating that the model should be loaded using 4-bit quantization. The bnb_4bit_quant_type argument is set to \"nf4\", specifying that the quantization data type for the bnb.nn.Linear4Bit layers should be NF4. The bnb_4bit_compute_dtype argument is set to compute_dtype, which is obtained using the getattr function.\n",
    "\n",
    "The getattr function is a built-in Python function that returns the value of a named attribute of an object. In this case, it is used to dynamically retrieve the value of the compute_dtype attribute from the torch module. The compute_dtype attribute is obtained using the string \"bfloat16\". This allows for flexibility in specifying the data type for computation.\n",
    "\n",
    "Overall, the provided code initializes a BitsAndBytesConfig object with specific configuration options for 4-bit quantization and computation data type. It demonstrates the use of the getattr function to dynamically retrieve attribute values based on string inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "compute_dtype = getattr(torch, \"bfloat16\")\n",
    "print(f'compute_dtype: {compute_dtype}' )\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *from_pretrained* method is a convenient way to load a pre-trained model for causal language modeling, with options to specify various configurations such as quantization, device mapping, and attention implementation.\n",
    "\n",
    "The *from_pretrained* method takes several arguments. The first argument is `pretrained_model_name_or_path`, which specifies the name or path of the pre-trained model to load. This argument is required.\n",
    "\n",
    "The other arguments are passed as keyword arguments (`**kwargs`). In the selected code, several keyword arguments are provided. Let's go through them one by one:\n",
    "- `trust_remote_code=True`: This argument specifies whether to trust remote code when loading the model. If set to True, the code will attempt to load the model from a remote source. If set to False, it will only load the model from a local source. In the selected code, it is set to True.\n",
    "- `quantization_config=bnb_config`: This argument specifies the quantization configuration for the model. Quantization is a technique used to reduce the memory footprint and improve the performance of the model. In the selected code, it is set to bnb_config.\n",
    "- `device_map={\"\": 0}`: This argument specifies the device map for the model. It maps device names to device IDs. In the selected code, it maps an empty string to device ID 0.\n",
    "- `torch_dtype=\"auto\"`: This argument specifies the torch data type for the model. In the selected code, it is set to \"auto\", which means the data type will be automatically determined.\n",
    "- `attn_implementation=\"eager\"`: This argument specifies the attention implementation for the model. Attention is a mechanism used in neural networks to focus on relevant parts of the input. In the selected code, it is set to \"eager\".\n",
    "\n",
    "The *from_pretrained* method performs several operations. It sets up the necessary configurations for loading the model. It also checks if the model has remote code and whether to trust it. If remote code is trusted, it dynamically loads the model class and registers it. If the model configuration is recognized, it retrieves the corresponding model class and loads the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          base_model_id, trust_remote_code=True, quantization_config=bnb_config, device_map={\"\": 0}, torch_dtype=\"auto\", attn_implementation=\"eager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a dataset that we can use to teach the model about new data. There are many from Hugging face, and you can build your own. Here we will use the Guanaco dataset from Tim Dettmers.\n",
    "\n",
    "> This dataset is a subset of the Open Assistant dataset, which you can find here: https://huggingface.co/datasets/OpenAssistant/oasst1/tree/main\n",
    "> This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\n",
    "\n",
    "* https://huggingface.co/datasets/timdettmers/openassistant-guanaco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The LoraConfig class provides a flexible way to configure the LoraModel and customize its behavior based on specific requirements and preferences.\n",
    "\n",
    " - `r`:his attribute represents the Lora attention dimension, also known as the \"rank\". This influences the model's capacity to learn and represent complex patterns. Higher rank allows more nuanced relationships in the data. The default is 8.\n",
    " - `lora_alpha`: This attribute represents the alpha parameter for Lora scaling. It has a default value of 8. This parameter allows for fine-tuning the balance between the original attention outputs and the adjustments introduced by Lora.\n",
    " - `lora_dropout`: This attribute represents the dropout probability for Lora layers. It has a default value of 0.0. Dropout is a form of regularization that helps prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time.\n",
    " - `bias`: The right balance of bias (and variance) is crucial for creating accurate and reliable models. Too much bias can lead to poor performance due to underfitting, while too little bias can cause overfitting, where the model performs well on training data but poorly on new, unseen data. It can be set to \"none\", \"all\", or \"lora_only\". If set to \"all\" or \"lora_only\", the corresponding biases will be updated during training.\n",
    " - `target_modules`: This attribute specifies the names of the modules to which the Lora adapter should be applied. A module refers to a component or layer of a neural network model. Modules are building blocks that perform specific operations or computations within the model. They can include components such as linear layers, convolutional layers, attention layers, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=8,  # Reduced from 16\n",
    "        lora_dropout=0.05,\n",
    "        r=8,  # Reduced from 16\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\"q_proj\", \"k_proj\"] #, \"v_proj\", \"fc2\",\"fc1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *prepare_model_for_kbit_training* function is a convenient wrapper that handles various steps to prepare a model for training in the transformers library. It takes care of freezing base model layers, casting parameters to the appropriate precision, and enabling gradient checkpointing if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes a *TrainingArguments* object with various parameters that control the training process. These parameters define settings such as output directory, evaluation strategy, batch sizes, learning rate, optimizer, and more. The specific values chosen for these parameters will depend on the requirements of the training task and the available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"./phi1-results1\",\n",
    "        eval_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=3,  # Reduced from 12 to 3\n",
    "        per_device_eval_batch_size=1,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=100,\n",
    "        logging_steps=25, \n",
    "        learning_rate=1e-4,\n",
    "        eval_steps=50,\n",
    "        optim='paged_adamw_8bit',\n",
    "        bf16=True, #change to fp16 if are using an older GPU\n",
    "        num_train_epochs=1,  # Reduced from 3 to 1\n",
    "        warmup_steps=100,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        use_cpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *SFTTrainer* constructor performs various checks and configurations based on the provided arguments. It handles cases where the model is a string identifier, initializes the `PeftModel` if *peft_config* is provided, sets the tokenizer if not specified, handles packing-related arguments, and prepares the datasets based on the specified options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=1024,  # Reduced from 1024\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "tokenizer.save_pretrained(\"./fine-tuned-model\")\n",
    "model.save_pretrained(\"./fine-tuned-model\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModel, AutoTokenizer\n",
    "\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./fine-tuned-model\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned-model\", use_fast=True)\n",
    "\n",
    "# Ensure padding and EOS token settings are the same as during training\n",
    "fine_tuned_tokenizer.padding_side = 'right'\n",
    "fine_tuned_tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "duration = 0.0\n",
    "total_length = 0\n",
    "prompt = []\n",
    "#prompt.append(\"Write the recipe for a chicken curry with coconut milk.\")\n",
    "#prompt.append(\"Translate into French the following sentence: I love bread and cheese!\")\n",
    "prompt.append(\"### Human: Cite 20 famous people.### Assistant:\")\n",
    "#prompt.append(\"Where is the moon right now?\")\n",
    "\n",
    "for i in range(len(prompt)):\n",
    "  model_inputs = fine_tuned_tokenizer(prompt[i], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "  start_time = time.time()\n",
    "  output = fine_tuned_model.generate(**model_inputs, max_length=500)[0]\n",
    "  duration += float(time.time() - start_time)\n",
    "  total_length += len(output)\n",
    "  tok_sec_prompt = round(len(output)/float(time.time() - start_time),3)\n",
    "  print(\"Prompt --- %s tokens/seconds ---\" % (tok_sec_prompt))\n",
    "  print(print_gpu_utilization())\n",
    "  print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
